{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI0CCRYGbsLX"
      },
      "source": [
        "# 第6回講義 宿題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhZjl_2gbsLf"
      },
      "source": [
        "## 課題\n",
        "RNNを用いてIMDbのsentiment analysisを実装してみましょう．\n",
        "\n",
        "ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBcfs2VybsLj"
      },
      "source": [
        "## 目標値\n",
        "F値：0.85"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZj-U7SvbsLl"
      },
      "source": [
        "## ルール\n",
        "- 以下のセルで指定されている`x_train`, `t_train`以外の学習データは使わないでください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9ZJX2TbsLo"
      },
      "source": [
        "## 提出方法\n",
        "- 2つのファイルを提出していただきます．\n",
        "  1. テストデータ `x_test` に対する予測ラベルを`submission_pred.csv`として保存し，Omnicampusの宿題から「第6回 回帰結合型ニューラルネットワーク」を選択して提出してください．\n",
        "  2. それに対応するpythonのコードを`submission_code.py`として保存し，Omnicampusの宿題から「第6回 回帰結合型ニューラルネットワーク (code)」を選択して提出してください．\n",
        "    - セルに書いたコードを.py形式で保存するためには%%writefileコマンドなどを利用してください．\n",
        "    - writefileコマンドではファイルの保存のみが行われセル内のpythonコード自体は実行されません．そのため，実際にコードを走らせる際にはwritefileコマンドをコメントアウトしてください．\n",
        "\n",
        "\n",
        "- コードの内容を変更した場合は，1と2の両方を提出し直してください．\n",
        "\n",
        "- なお採点は1で行い，2はコードの確認用として利用します．(成績優秀者はコード内容を公開させていただくかもしれません)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejdA6CESbsLs"
      },
      "source": [
        "## 評価方法\n",
        "\n",
        "- 予測ラベルの`t_test`に対するF値で評価します．\n",
        "- 即時採点しLeader Boardを更新します．（採点スケジュールは別アナウンス）\n",
        "- 締切時の点数を最終的な評価とします．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyYaymAQ4Wey"
      },
      "source": [
        "### ドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szesnftz4a1K",
        "outputId": "013821d6-4803-4b9d-f1b4-58ce8ca139fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjSgBf1ibsLw"
      },
      "source": [
        "## データの読み込み（このセルは修正しないでください）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X9ptijybsLz",
        "outputId": "544fc42c-dc1a-4c08-9f19-5d9772dad44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "単語種数: 88586\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "# 学習データ\n",
        "x_train = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/data/x_train.npy', allow_pickle=True)\n",
        "t_train = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/data/t_train.npy', allow_pickle=True)\n",
        "\n",
        "# 検証データを取る\n",
        "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.2, random_state=seed)\n",
        "    \n",
        "# テストデータ\n",
        "x_test = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/data/x_test.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "def text_transform(text: List[int], max_length=256):\n",
        "    # <BOS>はすでに1で入っている．<EOS>は2とする．\n",
        "    text = text[:max_length - 1] + [2]\n",
        "\n",
        "    return text, len(text)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, len_seq_list = [], [], []\n",
        "    \n",
        "    for sample in batch:\n",
        "        if isinstance(sample, tuple):\n",
        "            label, text = sample\n",
        "\n",
        "            label_list.append(label)\n",
        "        else:\n",
        "            text = sample.copy()\n",
        "            \n",
        "        text, len_seq = text_transform(text)\n",
        "        text_list.append(torch.tensor(text))\n",
        "        len_seq_list.append(len_seq)\n",
        "        \n",
        "    # NOTE: 宿題用データセットでは<PAD>は3です．\n",
        "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3).T, torch.tensor(len_seq_list)\n",
        "\n",
        "\n",
        "word_num = np.concatenate(np.concatenate((x_train, x_test))).max()\n",
        "print(f\"単語種数: {word_num}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIKgr01wbsL5"
      },
      "source": [
        "## 実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPhmbQtagOvH"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    [(t, x) for t, x in zip(t_train, x_train)],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    [(t, x) for t, x in zip(t_valid, x_valid)],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    x_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50RGuMWvND05"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        glorot = 6 / (in_dim + hid_dim*2)\n",
        "        self.W = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "    def function(self, h, x):\n",
        "        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n",
        "\n",
        "    def forward(self, x, len_seq_max=0, init_state=None):\n",
        "        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n",
        "        state = init_state\n",
        "        \n",
        "        if init_state is None:  # 初期値を設定しない場合は0で初期化する\n",
        "            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
        "\n",
        "        size = list(state.unsqueeze(0).size())\n",
        "        size[0] = 0\n",
        "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
        "\n",
        "        if len_seq_max == 0:\n",
        "            len_seq_max = x.size(0)\n",
        "        for i in range(len_seq_max):\n",
        "            state = self.function(state, x[i])\n",
        "            output = torch.cat([output, state.unsqueeze(0)])  # 出力系列の追加\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        glorot = 6/(in_dim + hid_dim*2)\n",
        "\n",
        "        self.W_i = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_i = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_f = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_o = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "        self.W_c = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_c = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "    def function(self, state_c, state_h, x):\n",
        "        i = torch.sigmoid(torch.matmul(torch.cat([state_h, x], dim=1), self.W_i) + self.b_i)\n",
        "        f = torch.sigmoid(torch.matmul(torch.cat([state_h, x], dim=1), self.W_f) + self.b_f)\n",
        "        o = torch.sigmoid(torch.matmul(torch.cat([state_h, x], dim=1), self.W_o) + self.b_o)\n",
        "        c = f*state_c + i*torch.tanh(torch.matmul(torch.cat([state_h, x], dim=1), self.W_c) + self.b_c)\n",
        "        h = o*torch.tanh(c)\n",
        "        return c, h\n",
        "\n",
        "    def forward(self, x, len_seq_max=0, init_state_c=None, init_state_h=None):\n",
        "        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n",
        "        state_c = init_state_c\n",
        "        state_h = init_state_h\n",
        "        if init_state_c is None:  # 初期値を設定しない場合は0で初期化する\n",
        "            state_c = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
        "        if init_state_h is None:  # 初期値を設定しない場合は0で初期化する\n",
        "            state_h = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
        "\n",
        "        size = list(state_h.unsqueeze(0).size())\n",
        "        size[0] = 0\n",
        "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
        "        \n",
        "        if len_seq_max == 0:\n",
        "            len_seq_max = x.size(0)\n",
        "        for i in range(len_seq_max):\n",
        "            state_c, state_h = self.function(state_c, state_h, x[i])\n",
        "            output = torch.cat([output, state_h.unsqueeze(0)])  # 出力系列の追加\n",
        "        return output"
      ],
      "metadata": {
        "id": "f4OdTrSEDCHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6j2soNLbsL7"
      },
      "outputs": [],
      "source": [
        "def torch_log(x):\n",
        "    return torch.log(torch.clamp(x, min=1e-10))\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, emb_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding_matrix = nn.Parameter(torch.rand((vocab_size, emb_dim),\n",
        "                                                        dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.embedding(x, self.embedding_matrix)\n",
        "\n",
        "\n",
        "class SequenceTaggingNet(nn.Module):\n",
        "    def __init__(self, word_num, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = Embedding(emb_dim, word_num)\n",
        "        self.rnn = RNN(emb_dim, hid_dim)\n",
        "        self.linear = nn.Linear(hid_dim, 1)\n",
        "\n",
        "    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n",
        "        h = self.emb(x)\n",
        "        h = self.rnn(h, len_seq_max, init_state)\n",
        "        if len_seq is not None:\n",
        "            # 系列が終わった時点での出力を取る必要があるので len_seq を元に集約する\n",
        "            h = h[len_seq - 1, list(range(len(x))), :]\n",
        "        else:\n",
        "            h = h[-1]\n",
        "        y = self.linear(h)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceTaggingNet3(nn.Module):\n",
        "    def __init__(self, word_num, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = Embedding(emb_dim, word_num)\n",
        "        self.lstm = LSTM(emb_dim, hid_dim)\n",
        "        self.linear = nn.Linear(hid_dim, 1)\n",
        "    \n",
        "    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n",
        "        h = self.emb(x)\n",
        "        h = self.lstm(h, len_seq_max, init_state)\n",
        "        if len_seq is not None:\n",
        "            # 系列が終わった時点での出力を取る必要があるので len_seq を元に集約する\n",
        "            h = h[len_seq - 1, list(range(len(x))), :]\n",
        "        else:\n",
        "            h = h[-1]\n",
        "        y = self.linear(h)\n",
        "        return y"
      ],
      "metadata": {
        "id": "09XvNQgaDLK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w3yjqqCl_Zb",
        "outputId": "0c594c09-f863-442b-ebb4-0204a915b258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0, Train Loss: 0.696, Valid Loss: 0.693, Validation F1: 0.332\n",
            "EPOCH: 1, Train Loss: 0.690, Valid Loss: 0.677, Validation F1: 0.559\n",
            "EPOCH: 2, Train Loss: 0.620, Valid Loss: 0.590, Validation F1: 0.688\n",
            "EPOCH: 3, Train Loss: 0.467, Valid Loss: 0.471, Validation F1: 0.776\n",
            "EPOCH: 4, Train Loss: 0.371, Valid Loss: 0.508, Validation F1: 0.769\n",
            "EPOCH: 5, Train Loss: 0.305, Valid Loss: 0.433, Validation F1: 0.822\n",
            "EPOCH: 6, Train Loss: 0.262, Valid Loss: 0.422, Validation F1: 0.816\n",
            "EPOCH: 7, Train Loss: 0.219, Valid Loss: 0.534, Validation F1: 0.805\n",
            "EPOCH: 8, Train Loss: 0.182, Valid Loss: 0.454, Validation F1: 0.829\n",
            "EPOCH: 9, Train Loss: 0.151, Valid Loss: 0.458, Validation F1: 0.834\n",
            "EPOCH: 10, Train Loss: 0.131, Valid Loss: 0.489, Validation F1: 0.823\n"
          ]
        }
      ],
      "source": [
        "emb_dim = 1024\n",
        "hid_dim = 1024\n",
        "n_epochs = 15\n",
        "device = 'cuda'\n",
        "\n",
        "best_f1_score = 0.0\n",
        "net = SequenceTaggingNet3(word_num, emb_dim, hid_dim)\n",
        "net.to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "\n",
        "    net.train()\n",
        "    n_train = 0\n",
        "    acc_train = 0\n",
        "    for label, line, len_seq in train_dataloader:\n",
        "        \n",
        "        net.zero_grad()\n",
        "\n",
        "        t = label.to(device)  # テンソルをGPUに移動\n",
        "        x = line.to(device) # ( batch, time )\n",
        "        len_seq.to(device)\n",
        "\n",
        "        h = net(x, torch.max(len_seq), len_seq)\n",
        "        y = torch.sigmoid(h).squeeze()\n",
        "    \n",
        "        loss = -torch.mean(t*torch_log(y) + (1 - t)*torch_log(1 - y))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # 勾配を絶対値1.0でクリッピングする\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        losses_train.append(loss.tolist())\n",
        "\n",
        "        n_train += t.size()[0]\n",
        "\n",
        "    # Valid\n",
        "    t_valid = []\n",
        "    y_pred = []\n",
        "    net.eval()\n",
        "    for label, line, len_seq in valid_dataloader:\n",
        "\n",
        "        # WRITE ME\n",
        "        t = label.to(device)  # テンソルをGPUに移動\n",
        "        x = line.to(device) # ( batch, time )\n",
        "        len_seq.to(device)\n",
        "\n",
        "        h = net(x, torch.max(len_seq), len_seq)\n",
        "        y = torch.sigmoid(h).squeeze()\n",
        "        \n",
        "        loss = -torch.mean(t*torch_log(y) + (1 - t)*torch_log(1 - y))\n",
        "\n",
        "        pred = y.round().squeeze()\n",
        "\n",
        "        t_valid.extend(t.tolist())\n",
        "        y_pred.extend(pred.tolist())\n",
        "\n",
        "        losses_valid.append(loss.tolist())\n",
        "\n",
        "    print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n",
        "        epoch,\n",
        "        np.mean(losses_train),\n",
        "        np.mean(losses_valid),\n",
        "        f1_score(t_valid, y_pred, average='macro')\n",
        "    ))\n",
        "\n",
        "    if f1_score(t_valid, y_pred, average='macro') > best_f1_score:\n",
        "        best_f1_score = f1_score(t_valid, y_pred, average='macro')\n",
        "        net.eval()\n",
        "\n",
        "        y_pred = []\n",
        "        for _, line, len_seq in test_dataloader:\n",
        "\n",
        "            x = line.to(device)\n",
        "            len_seq.to(device)\n",
        "\n",
        "            h = net(x, torch.max(len_seq), len_seq)\n",
        "            y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "            pred = y.round().squeeze()  # 0.5以上の値を持つ要素を正ラベルと予測する\n",
        "\n",
        "            y_pred.extend(pred.tolist())\n",
        "\n",
        "\n",
        "        submission = pd.Series(y_pred, name='label')\n",
        "        submission.to_csv('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/submission_pre.csv', header=True, index_label='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ8xtUcHj6IR"
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "\n",
        "y_pred = []\n",
        "for _, line, len_seq in test_dataloader:\n",
        "\n",
        "    x = line.to(device)\n",
        "    len_seq.to(device)\n",
        "\n",
        "    h = net(x, torch.max(len_seq), len_seq)\n",
        "    y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "    pred = y.round().squeeze()  # 0.5以上の値を持つ要素を正ラベルと予測する\n",
        "\n",
        "    y_pred.extend(pred.tolist())\n",
        "\n",
        "\n",
        "submission = pd.Series(y_pred, name='label')\n",
        "submission.to_csv('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/submission_pred.csv', header=True, index_label='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-MRCNboHZNd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}